# Phase 4: Implement dbt Project for Data Transformations

## Summary

This PR implements **Phase 4: dbt Project** of the CVE Advisory Pipeline prototype. It delivers a production-quality data transformation layer that converts raw source observations into actionable advisory decisions using a rule-based decision engine.

## What's Implemented

### ğŸ“ Project Structure

```
dbt_project/
â”œâ”€â”€ dbt_project.yml          # Project configuration
â”œâ”€â”€ profiles.yml             # DuckDB connection config
â”œâ”€â”€ README.md                # Comprehensive documentation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources.yml          # Raw table definitions
â”‚   â”œâ”€â”€ staging/             # Cleaning & validation (views)
â”‚   â”‚   â”œâ”€â”€ stg_echo_advisories.sql
â”‚   â”‚   â”œâ”€â”€ stg_echo_csv.sql
â”‚   â”‚   â”œâ”€â”€ stg_nvd_observations.sql
â”‚   â”‚   â”œâ”€â”€ stg_osv_observations.sql
â”‚   â”‚   â””â”€â”€ staging.yml
â”‚   â”œâ”€â”€ intermediate/        # Enrichment & conflict resolution (tables)
â”‚   â”‚   â”œâ”€â”€ int_source_observations.sql
â”‚   â”‚   â”œâ”€â”€ int_enriched_advisories.sql
â”‚   â”‚   â”œâ”€â”€ int_decision_inputs.sql
â”‚   â”‚   â””â”€â”€ intermediate.yml
â”‚   â””â”€â”€ marts/               # Final outputs (tables)
â”‚       â”œâ”€â”€ mart_advisory_decisions.sql
â”‚       â”œâ”€â”€ mart_advisory_current.sql
â”‚       â””â”€â”€ marts.yml
â”œâ”€â”€ macros/
â”‚   â””â”€â”€ generate_explanation.sql
â””â”€â”€ tests/
    â”œâ”€â”€ assert_all_advisories_have_decisions.sql
    â”œâ”€â”€ assert_no_duplicate_decisions.sql
    â””â”€â”€ assert_final_states_have_evidence.sql
```

### ğŸ—ï¸ Architecture

**Medallion Pattern**: Raw â†’ Staging â†’ Intermediate â†’ Marts

1. **Staging Layer** (Views)
   - Validates CVE ID formats
   - Normalizes field names and values
   - Derives severity from CVSS scores
   - Flags data quality issues

2. **Intermediate Layer** (Tables)
   - Unifies all sources into common schema
   - Applies source priority for conflict resolution
   - Aggregates signals per advisory
   - Calculates confidence scores

3. **Mart Layer** (Tables)
   - Executes rule-based decision engine
   - Generates human-readable explanations
   - Tracks evidence and dissenting sources
   - Produces customer-facing output

### ğŸ¯ Decision Engine

The rule engine implements a priority-based decision chain:

| Priority | Rule | Trigger Condition | Resulting State |
|----------|------|-------------------|-----------------|
| 0 | CSV Override | Analyst marked as not_applicable | `not_applicable` (final) |
| 1 | NVD Rejected | CVE rejected by NVD | `not_applicable` (final) |
| 2 | Upstream Fix | Fix available with version | `fixed` (final) |
| 5 | Under Investigation | No signals from any source | `under_investigation` (non-final) |
| 6 | Pending Upstream | Default case | `pending_upstream` (non-final) |

**First match wins** - rules are evaluated in priority order.

### ğŸ”§ Key Design Decisions

#### 1. Source Priority for Conflict Resolution
When sources disagree, this hierarchy applies:
- **echo_csv** (0) - Internal analyst decisions override everything
- **nvd** (1) - Authoritative for CVE metadata
- **osv** (2) - Best for fix availability
- **echo_data** (3) - Base data, lowest priority

#### 2. Advisory Identifier
Advisories use composite key: `package_name:CVE_ID`

This ensures we track CVE impact at package granularity, not just CVE-level.

#### 3. Materialization Strategy
- **Staging**: Views (lightweight, always fresh)
- **Intermediate**: Tables (reused by multiple marts)
- **Marts**: Tables (optimized for consumption)

#### 4. State Classification
- **final**: Decision is definitive (fixed, not_applicable, wont_fix)
- **non_final**: May change as new signals arrive (pending_upstream, under_investigation)

### ğŸ§ª Quality Assurance

#### Schema Tests
All models include YAML-defined tests:
- Uniqueness constraints on keys
- Not null requirements
- Accepted value checks
- Referential integrity

#### Data Tests
Custom SQL tests ensure:
- **Completeness**: All source advisories have decisions
- **Uniqueness**: No duplicate decisions per advisory
- **Quality**: Final states have supporting evidence

#### Documentation
- Model-level descriptions
- Column-level documentation
- Lineage tracking via dbt DAG

### ğŸ”Œ Integration Points

**Consumes from Phase 2-3**:
- `raw_echo_advisories` (Python ingestion)
- `raw_echo_csv` (Python ingestion)
- `raw_nvd_observations` (Python ingestion)
- `raw_osv_observations` (Python ingestion)

**Produces**:
- `mart_advisory_current` - Primary output table
- `mart_advisory_decisions` - Detailed decision audit trail

**Orchestration**:
```python
# Called from Python pipeline
subprocess.run(['dbt', 'run'], cwd='dbt_project', check=True)
subprocess.run(['dbt', 'test'], cwd='dbt_project', check=True)
```

### ğŸ“Š Output Schema

The primary output table `mart_advisory_current` provides:

```sql
advisory_id          -- 'package_name:CVE-2024-1234'
cve_id               -- 'CVE-2024-1234'
package_name         -- 'example-package'
state                -- 'fixed' | 'not_applicable' | 'pending_upstream' | ...
state_type           -- 'final' | 'non_final'
fixed_version        -- '1.2.3' (if applicable)
confidence           -- 'high' | 'medium' | 'low'
explanation          -- "Fixed in version 1.2.3. Fix available from upstream."
reason_code          -- 'UPSTREAM_FIX'
decision_rule        -- 'R2:upstream_fix'
evidence             -- JSON with supporting signals
contributing_sources -- ['echo_data', 'nvd', 'osv']
dissenting_sources   -- ['osv'] (if conflicts exist)
staleness_score      -- Float (currently 0.0, placeholder)
decided_at           -- Timestamp
run_id               -- Pipeline run identifier
```

## Known Limitations

1. **Staleness Scoring**: Simplified to 0.0. Production would calculate based on last observation time.

2. **Distro Rules**: R3 (distro_not_affected) and R4 (distro_wont_fix) not implemented due to lack of distro-specific data sources.

3. **SCD2 History**: `advisory_state_history` table exists but not yet populated by dbt snapshots (future enhancement).

4. **Conflict Tracking**: Dissenting sources only tracked in simple cases (CSV override vs OSV fix).

## Testing

### How to Run
```bash
cd advisory_pipeline/dbt_project

# Compile and run all models
dbt run

# Execute all tests
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

### Prerequisites
1. Python pipeline (Phase 2-3) must run at least once to create raw tables
2. DuckDB database must exist at `advisory_pipeline.duckdb`
3. Set `PIPELINE_RUN_ID` environment variable (optional, defaults to 'manual_run')

## Follow-up Work

Future phases could add:
- [ ] dbt snapshots for SCD Type 2 state history
- [ ] Temporal staleness calculation
- [ ] Additional mart views (high severity filter, recent changes, etc.)
- [ ] Incremental model support for performance
- [ ] Enhanced conflict resolution with weighted voting

## Documentation

See [advisory_pipeline/dbt_project/README.md](advisory_pipeline/dbt_project/README.md) for:
- Detailed architecture explanation
- Running instructions
- Troubleshooting guide
- Integration patterns

## Checklist

- [x] All dbt models implemented (staging, intermediate, marts)
- [x] Source definitions complete
- [x] Macros for reusable logic
- [x] Schema tests on all models
- [x] Custom data quality tests
- [x] Comprehensive README
- [x] Integration with existing Python pipeline
- [x] Clean, production-ready SQL
- [x] Proper materialization strategies

---

**Reviewer Notes**: This implementation follows dbt best practices with clear separation of concerns, documented assumptions, and extensible design. The rule engine is deterministic and fully explainable. All code is production-quality despite being a prototype.

**Branch**: `feature/phase4-dbt-project`
**Files Changed**: 20 files, 1,146 insertions

**To Create PR**: Visit https://github.com/omer-ginosar/hush/compare/main...feature/phase4-dbt-project
